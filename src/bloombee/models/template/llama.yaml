####

# --- Basic Identifiers ---
model_name: llama                  # Output directory name, import paths
class_prefix: Llama                # Class name prefix (e.g., OptimizedLlamaAttention)
hf_model_key: llama                # HuggingFace model key (transformers.models.{hf_model_key})
config_class: LlamaConfig          # HuggingFace config class name

# ============================================================
# block.py Template Parameters
# ============================================================

model_name: llama                  # {model_name}/
class_prefix: Llama               # Optimized{{class_prefix}}Attention
hf_model_key: llama               # transformers.models.{{hf_model_key}}.modeling_{{hf_model_key}}
config_class: LlamaConfig

flex_module_import: bloombee.models.llama.flex_llama
attention_class: FLEX_LlamaAttention
mlp_class: FLEX_LlamaMLP
norm_class: FLEX_LlamaRMSNorm
decoder_base_class: LlamaDecoderLayer

weight_download_module: bloombee.flexgen_utils.llama_config #path for dowload function eg. from bloombee.flexgen_utils.llama_config import download_llama_weights
weight_download_fn: download_llama_weights

needs_bloom_to_llama_reorder: true  #generate _reorder_cache_from_bloom_to_llama() and _reorder_cache_from_llama_to_bloom()
uses_rotary: true # use apply_rotary_pos_emb and insert _optimized_apply_rotary() at OptimizedAttention
prepare_mask_with_hf: true        # Changed to true TO USE _prepare_4d_causal_attention_mask IN WrappedBlock.forward(), not flexgen mask

# New for tokenizer support
default_model_name: llama-7b
hf_tokenizer_prefix: huggyllama

# custom implementation
shared_impl_snippet: /home/twei11/new/BloomBee/src/bloombee/models/template/decoder_shared_impl.pyfrag #custom decoder


# ============================================================
# config.py Template Parameters
# ============================================================

# --- HuggingFace Attention Class ---
hf_attention_class: LlamaAttention

# --- Block Prefix in Model ---
block_prefix: model.layers

# --- GQA Support ---
has_num_key_value_groups: true

# --- DHT Suffix ---
dht_suffix: "-hf"

# --- Config Overrides ---
config_overrides:
  pretraining_tp: 1
  use_cache: true

# config_override_comments:
#   pretraining_tp: "This may give less accurate results but it doesn't matter if we use quantization"
#   use_cache: "use_cache=False leads to identical results but is slower and not supported by Petals"

# ============================================================
# model.py Template Parameters
# ============================================================

# --- HuggingFace Model Classes ---
hf_model_class: LlamaModel                           # Base model class
hf_pretrained_class: LlamaPreTrainedModel            # PreTrained base class
hf_causal_lm_class: LlamaForCausalLM                 # Causal LM class
hf_seq_cls_class: LlamaForSequenceClassification    # Sequence classification class

# --- Model Architecture Attributes ---
# These vary between model architectures (e.g., GPT-2 uses 'h', LLaMA uses 'layers')
layers_attr: layers              # Attribute name for transformer layers (self.layers)
embed_tokens_attr: embed_tokens  # Attribute name for token embeddings (self.embed_tokens)
final_norm_attr: norm            # Attribute name for final layer norm (self.norm)

# --- Feature Flags ---
has_pretraining_tp: true         # Whether model has pretraining_tp config option