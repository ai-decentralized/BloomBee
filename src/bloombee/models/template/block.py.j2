{% set M = class_prefix %}
"""
{{ M }} intermediate layer
Auto-generated from templates/block.py.j2 with spec for '{{ model_name }}'.
Based on https://github.com/huggingface/transformers/blob/main/src/transformers/models/{{ hf_model_key }}/modeling_{{ hf_model_key }}.py
"""
from typing import Optional, Tuple
import os
import sys
import threading
import numpy as np
import torch
import torch.nn as nn

{% if prepare_mask_with_hf %}
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
{% endif %}
from transformers.models.{{ hf_model_key }}.modeling_{{ hf_model_key }} import (
    {{ config_class }},
)
from transformers import AutoTokenizer

from bloombee.flexgen_utils.task import Task
from bloombee.utils.cuda_graphs import make_inference_graphed_callable
from bloombee.flexgen_utils.ExecutionEnv import ExecutionEnv
from bloombee.flexgen_utils.policy import Policy
from bloombee.flexgen_utils.pytorch_backend import fix_recursive_import, TorchTensor, TorchDevice
from bloombee.flexgen_utils.utils import ValueHolder, array_1d, array_2d, array_3d

from {{ flex_module_import }} import (
    {{ attention_class }},
    {{ mlp_class }},
    {{ decoder_base_class }},
    DUMMY_WEIGHT,
    {% if uses_rotary %}apply_rotary_pos_emb,{% endif %}
    {{ norm_class }},
)

from {{ weight_download_module }} import {{ weight_download_fn }}

fix_recursive_import()


# ---------------------------------------------------------------------------
# Global tokenizer singleton - avoid creating duplicate tokenizers for each layer
# ---------------------------------------------------------------------------
_global_tokenizer = None
_tokenizer_lock = threading.Lock() if 'threading' in sys.modules else None


def get_global_tokenizer(model_name='{{ default_model_name | default("llama-7b") }}'):
    """Get globally shared tokenizer, initialize only once"""
    global _global_tokenizer
    if _global_tokenizer is None:
        try:
            # Remove -hf suffix if present
            clean_model_name = model_name.replace('-hf', '') if model_name.endswith('-hf') else model_name
            hf_model_name = f"{{ hf_tokenizer_prefix | default("huggyllama") }}/{clean_model_name}"

            _global_tokenizer = AutoTokenizer.from_pretrained(
                hf_model_name,
                padding_side="left",
                legacy=False
            )
            _global_tokenizer.pad_token = '[PAD]'
        except Exception as e:
            _global_tokenizer = None
    return _global_tokenizer


def get_test_inputs(prompt_len, num_prompts, tokenizer):
    """Simplify test_inputs generation to reduce tokenizer call overhead"""
    pad_token_id = getattr(tokenizer, 'pad_token_id', 0)
    if pad_token_id is None:
        pad_token_id = 0
    simple_input_ids = [pad_token_id]
    return (simple_input_ids,) * num_prompts


class Optimized{{ M }}Attention({{ attention_class }}):
    """
    Thin wrapper around the model-specific Attention to:
      - normalize/derive position_ids if absent
      - keep FlexGen buffers wired
      - optionally graph the rotary op (if enabled)
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.temp_hidden_states = ValueHolder()
        self._rotary_graph = None

    {% if uses_rotary %}
    def _optimized_apply_rotary(self, query_states, key_states, cos, sin):
        if self._rotary_graph is None:
            self._rotary_graph = make_inference_graphed_callable(
                apply_rotary_pos_emb, sample_args=(query_states, key_states, cos, sin)
            )
        return self._rotary_graph(query_states, key_states, cos, sin)
    {% endif %}

    def forward(  # pyright: ignore[reportIncompatibleMethodOverride]
        self,
        hidden_states: torch.Tensor,
        cache_read_buf: ValueHolder,
        weight_read_buf: ValueHolder,
        cache_write_buf: ValueHolder,
        k: Optional[int] = 0,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        generated_tokens_num: int = 0,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        output_attentions = False
        assert not output_attentions

        # Minimal fallback for position_ids (trust caller if provided)
        if position_ids is None:
            past_seen = past_key_value[0].shape[2] if past_key_value is not None else 0
            position_ids = torch.arange(
                past_seen, past_seen + hidden_states.shape[1],
                device=hidden_states.device, dtype=torch.long
            ).unsqueeze(0)

        # Derive start_position from position_ids (supports [B, T], [T], or scalar)
        # Optimized: Avoid .item() CPU-GPU sync by using direct indexing
        if position_ids.dim() == 2:
            start_position = position_ids[0, 0]
        elif position_ids.dim() == 1:
            start_position = position_ids[0]
        elif position_ids.dim() == 0:
            start_position = position_ids
        elif position_ids.numel() == 0 or generated_tokens_num == 0:
            start_position = 0
        else:
            start_position = 0

        self.temp_hidden_states.val = super(Optimized{{ M }}Attention, self).forward(
            hidden_states, cache_read_buf, weight_read_buf, attention_mask, cache_write_buf, start_position, k
        )
        return self.temp_hidden_states.val, None, None


class Optimized{{ M }}DecoderLayer({{ decoder_base_class }}):
    """
    Model-specific decoder leaf that wires FlexGen components (attention, MLP, norms)
    and provides common helpers.
    """
    def __init__(self, config: {{ config_class }}, layer_id: int, env: ExecutionEnv, policy: Policy, weight_home: array_1d, path: str):
        nn.Module.__init__(self)
        self.layer_id = layer_id
        self.config = config
        self.env = env
        self.policy = policy

        self.self_attn = Optimized{{ M }}Attention(config=config, env=env, policy=policy, layer_id=self.layer_id)
        self.mlp = {{ mlp_class }}(config=config, env=env, policy=policy, layer_id=self.layer_id)

        self.input_layernorm = {{ norm_class }}(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = {{ norm_class }}(config.hidden_size, eps=config.rms_norm_eps)

        self.pre_attn_graph = None
        self.post_attn_graph = None

        self.llama_config = config
        self.path = path
        self.num_gpu_batches = policy.num_gpu_batches

        self.layers = [self.self_attn, self.mlp]
        self.num_layers = len(self.layers)

        # Where activations live
        if self.policy.act_gpu_percent == 100:
            self.act_home = self.env.gpu
        elif self.policy.act_cpu_percent == 100:
            self.act_home = self.env.cpu
        elif self.policy.act_disk_percent == 100:
            self.act_home = self.env.disk
        else:
            raise NotImplementedError()

        # Conditionally create CUDA streams only if CUDA is available
        if torch.cuda.is_available():
            self.load_weight_stream = torch.cuda.Stream()
            self.load_cache_stream = torch.cuda.Stream()
            self.store_cache_stream = torch.cuda.Stream()
        else:
            self.load_weight_stream = None
            self.load_cache_stream = None
            self.store_cache_stream = None

        # FlexGen buffers
        num_layers, num_gpu_batches = self.num_layers, self.policy.num_gpu_batches
        self.cache_home = array_2d(num_layers, num_gpu_batches, ValueHolder)
        self.cache_read_buf = array_2d(num_layers, num_gpu_batches, ValueHolder)
        self.cache_write_buf = array_2d(num_layers, num_gpu_batches, ValueHolder)
        self.weight_read_buf = array_1d(num_layers, ValueHolder)
        self.attention_mask = array_1d(num_gpu_batches, ValueHolder)

        # Task binding
        self.task = None

        # Use globally shared tokenizer
        self._cached_tokenizer = None

        # Improved Task management
        self._cached_task = None
        self._is_initialized = False
        self._test_inputs_cache = {}

        # Cache frequently used calculation results
        self._last_prompt_len = None
        self._last_gen_len = None

        # Object reuse and caching strategy
        self._cached_torch_device = None
        self._cached_hidden_array = None
        self._last_gen_len_for_hidden = None
        self._cache_cleared = False

        # GPU stream management optimization
        self._streams_initialized = False

        # Rolling buffer for output_ids
        self._cached_output_ids = None
        self._cached_output_ids_shape = None
        self._output_ids_prompt_initialized = False

        self.temp_hidden = ValueHolder()

        self.init_all_weights()
        self._init_gpu_streams_if_needed()

    def _get_tokenizer(self):
        """Use globally shared tokenizer to avoid duplicate creation"""
        if self._cached_tokenizer is None:
            model_name = getattr(self.llama_config, 'name', '{{ default_model_name | default("llama-7b") }}')
            clean_model_name = model_name.replace('-hf', '') if model_name.endswith('-hf') else model_name
            self._cached_tokenizer = get_global_tokenizer(clean_model_name)
        return self._cached_tokenizer

    def _get_cached_test_inputs(self, prompt_len, num_prompts):
        """Cache test_inputs results to avoid duplicate calculations"""
        cache_key = (prompt_len, num_prompts)
        if cache_key not in self._test_inputs_cache:
            tokenizer = self._get_tokenizer()
            if tokenizer is not None:
                self._test_inputs_cache[cache_key] = get_test_inputs(
                    prompt_len, num_prompts, tokenizer
                )
            else:
                self._test_inputs_cache[cache_key] = ([0],) * num_prompts
        return self._test_inputs_cache[cache_key]

    def _should_rebuild_task(self, max_new_tokens, actual_prompt_len):
        """Simplify Task rebuild logic"""
        if self._cached_task is None:
            return True
        if (self._cached_task.gen_len != max_new_tokens or
            self._cached_task.prompt_len != actual_prompt_len):
            return True
        return False

    def _should_force_cache_clear(self):
        """Determine if cache needs to be force cleared"""
        return (self._cached_task is None or
                self._last_prompt_len != self._cached_task.prompt_len or
                self._last_gen_len != self._cached_task.gen_len)

    def _init_gpu_streams_if_needed(self):
        """Lazy initialization of GPU streams"""
        if not self._streams_initialized:
            if torch.cuda.is_available():
                if not hasattr(self, 'load_weight_stream') or self.load_weight_stream is None:
                    self.load_weight_stream = torch.cuda.Stream()
                if not hasattr(self, 'load_cache_stream') or self.load_cache_stream is None:
                    self.load_cache_stream = torch.cuda.Stream()
                if not hasattr(self, 'store_cache_stream') or self.store_cache_stream is None:
                    self.store_cache_stream = torch.cuda.Stream()
            else:
                if not hasattr(self, 'load_weight_stream'):
                    self.load_weight_stream = None
                if not hasattr(self, 'load_cache_stream'):
                    self.load_cache_stream = None
                if not hasattr(self, 'store_cache_stream'):
                    self.store_cache_stream = None
            self._streams_initialized = True

    def set_task(self, task):
        self.task = task
        for l in self.layers:
            l.set_task(task)

    def init_all_weights(self):
        self.weight_home = array_1d(self.num_layers, ValueHolder)
        for j in range(self.num_layers):
            self.init_weight(j)

    def init_weight(self, j):
        # Get model name from config
        if hasattr(self.llama_config, 'name') and self.llama_config.name:
            model_name = self.llama_config.name
        elif hasattr(self.llama_config, '_name_or_path') and self.llama_config._name_or_path:
            model_name = os.path.basename(self.llama_config._name_or_path.rstrip('/'))
        else:
            model_name = "{{ default_model_name | default("llama-30b") }}"

        if not model_name or model_name == '.':
            model_name = "{{ default_model_name | default("llama-30b") }}"

        # Remove -hf suffix if present
        if model_name.endswith('-hf'):
            model_name = model_name[:-3]

        self.llama_config.name = model_name
        expanded_path = os.path.abspath(os.path.expanduser(
            os.path.join(self.path, f"{model_name}-np")))
        check_path = os.path.join(expanded_path, "embed_tokens.weight")
        if not os.path.exists(check_path) and DUMMY_WEIGHT not in check_path:
            {{ weight_download_fn }}(self.llama_config.name, self.path)

        self.layers[j].init_weight(self.weight_home[j], expanded_path)

    def _optimized_input_layernorm(self, hidden_states):
        if self.pre_attn_graph is None:
            self.pre_attn_graph = make_inference_graphed_callable(
                self.input_layernorm.forward, sample_args=(hidden_states,)
            )
        return self.pre_attn_graph(hidden_states)

    def _optimized_output_layernorm(self, hidden_states):
        if self.post_attn_graph is None:
            self.post_attn_graph = make_inference_graphed_callable(
                self.post_attention_layernorm.forward, sample_args=(hidden_states,)
            )
        return self.post_attn_graph(hidden_states)

    def update_attention_mask(self, generated_tokens_num: int, k: int, mask_length: int):
        """
        FlexGen boolean mask builder/expander.
        """
        if generated_tokens_num > 0:
            mask = self.attention_mask[k]
            if mask.val is not None:
                mask.val = mask.val.device.extend_attention_mask(mask.val, [True])
                return

        gpu_batch_size = self.policy.gpu_batch_size
        attention_compute = (self.env.cpu if self.policy.cpu_cache_compute else self.env.gpu)
        val = attention_compute.allocate((gpu_batch_size, mask_length), bool)
        val.load_from_np(np.ones((gpu_batch_size, mask_length), dtype=bool))
        self.attention_mask[k].store(val)

    {% if shared_impl_snippet %}
{{ shared_impl_snippet | safe }}
    {% else %}
    def forward(
        self,
        hidden_states: torch.Tensor,
        *args,
        max_new_tokens: int = 1,
        do_sample: bool = True,
        temperature: float = 0.6,
        stop: Optional[int] = None,
        cut_gen_len: Optional[int] = None,
        top_p: float = 0.9,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
        verbose: int = 0,
        **kwargs,
    ):
        num_prompts = 1
        actual_prompt_len = hidden_states.shape[1] if hidden_states.shape[1] > 0 else 1
        prompt_len, gen_len, cut_gen_len = actual_prompt_len, max_new_tokens, max_new_tokens

        if self._should_rebuild_task(max_new_tokens, actual_prompt_len):
            inputs = self._get_cached_test_inputs(prompt_len, num_prompts)
            self._cached_task = Task(
                inputs=inputs,
                prompt_len=len(inputs[0]),
                gen_len=max_new_tokens,
                cut_gen_len=cut_gen_len,
                do_sample=do_sample,
                temperature=temperature,
                stop=stop,
                top_p=top_p
            )
            self._last_prompt_len = actual_prompt_len
            self._last_gen_len = max_new_tokens
            self._output_ids_prompt_initialized = False
            if not self._is_initialized:
                self._is_initialized = True

        task = self._cached_task
        num_layers = self.num_layers
        num_gpu_batches = self.num_gpu_batches
        prompt_len, gen_len = task.prompt_len, task.gen_len

        # Rolling buffer for output_ids
        target_shape = (num_prompts, prompt_len + gen_len)
        if self._cached_output_ids is None or self._cached_output_ids_shape != target_shape:
            self._cached_output_ids = np.ones(target_shape, dtype=np.int64)
            self._cached_output_ids_shape = target_shape
            self._output_ids_prompt_initialized = False

        if not self._output_ids_prompt_initialized:
            self._cached_output_ids[:, :prompt_len] = np.asarray(task.inputs)
            self._output_ids_prompt_initialized = True

        self.output_ids = self._cached_output_ids

        # Smart cache clearing
        if not self._cache_cleared or self._should_force_cache_clear():
            for j in range(num_layers):
                for k in range(num_gpu_batches):
                    self.cache_read_buf[j][k].clear()
                    self.cache_write_buf[j][k].clear()
            for j in range(num_layers):
                self.weight_read_buf[j].clear()
            for k in range(num_gpu_batches):
                self.attention_mask[k].clear()
            self._cache_cleared = True

        # Smart hidden array reuse
        if (self._cached_hidden_array is None or
            self._last_gen_len_for_hidden != gen_len):
            self.hidden = array_3d(gen_len, num_layers, num_gpu_batches, ValueHolder)
            self._cached_hidden_array = self.hidden
            self._last_gen_len_for_hidden = gen_len
        else:
            self.hidden = self._cached_hidden_array

        # TorchDevice object reuse
        data = hidden_states
        if (self._cached_torch_device is None or
            self._cached_torch_device.name != str(data.device)):
            self._cached_torch_device = TorchDevice(data.device)
        device = self._cached_torch_device

        tensor_data = TorchTensor(shape=data.shape, data=data, dtype=data.dtype, device=device)
        self.hidden[0][0][0].store(tensor_data)

        self.task = task
        self.set_task(task)
        if self.policy.cpu_cache_compute:
            self.env.cpu.init_attention_compute_workspace(self.config, self.task, self.policy)

        overlap = self.policy.overlap if hasattr(self.policy, 'overlap') else False

        if position_ids is not None and position_ids.numel() > 0:
            current_position = position_ids.flatten()[0]
        else:
            current_position = 0

        i = current_position

        for k in range(self.num_gpu_batches):
            if i == 0:
                mask_length = hidden_states.shape[1]
            else:
                mask_length = i + 1
            self.update_attention_mask(0, k, mask_length)

        for j in range(self.num_layers):
            for k in range(self.num_gpu_batches):
                self.load_weight(i, j, k, overlap=False)

        final_outputs = []
        generated_tokens_num = 0
        if position_ids is not None and position_ids.numel() > 0:
            generated_tokens_num = position_ids.flatten()[-1] - self.task.prompt_len + 1

        for k in range(self.num_gpu_batches):
            for j in range(self.num_layers):
                if j == 0 and past_key_value is not None:
                    past_k_new, past_v_new = self._normalize_past_for_flexgen(past_key_value, hidden_states)
                    self.cache_read_buf[0][0].store((past_k_new, past_v_new))

                layer_output = self.compute_layer(
                    i=current_position, j=j, k=k,
                    position_ids=position_ids,
                    generated_tokens_num=generated_tokens_num
                )

                if j == 0:
                    k_new, v_new = self.cache_write_buf[0][0].pop()
                    key_t = self._to_torch_tensor(k_new)
                    val_t = self._to_torch_tensor(v_new)
                    key_backend = key_t.permute(1, 2, 0)
                    val_backend = val_t.permute(1, 0, 2)
                    past_key_value = (key_backend, val_backend)
                    self.cache_write_buf[0][0].store((k_new, v_new))

            if layer_output.data.requires_grad or layer_output.data._base is not None:
                final_outputs.append(layer_output.data.clone())
            else:
                final_outputs.append(layer_output.data)

        if len(final_outputs) == 1:
            out_hidden = final_outputs[0]
        else:
            out_hidden = torch.cat(final_outputs, dim=0)

        return (out_hidden, past_key_value)
    {% endif %}

    # ---------------- Helper methods ----------------

    def compute_layer(self, i: int, j: int, k: int, position_ids=None, generated_tokens_num: int = 0):
        if j == 1:
            self.hidden[0][j][k].val = self.temp_hidden.val

        self.layers[j].forward(
            hidden_states=self.hidden[0][j][k],
            cache_read_buf=self.cache_read_buf[j][k],
            weight_read_buf=self.weight_read_buf[j],
            cache_write_buf=self.cache_write_buf[j][k],
            k=k,
            attention_mask=self.attention_mask[k],
            position_ids=position_ids,
            generated_tokens_num=generated_tokens_num
        )
        self.temp_hidden.val = self.layers[j].temp_hidden_states.val
        return self.layers[j].temp_hidden_states.val

    def _normalize_past_for_flexgen(self, past_key_value: Tuple[torch.Tensor], hidden_states: torch.Tensor):
        """Normalize external past to FlexGen's expected (S, B*H, D)."""
        past_key, past_value = past_key_value
        if past_key.dim() == 3:
            bh, x1, x2 = past_key.shape
            b = hidden_states.shape[0]
            h = bh // b
            d = self.self_attn.head_dim
            s = x2 if x1 == d else x1

            if x1 == d and x2 == s:
                k_bhsd = past_key.permute(0, 2, 1)
            else:
                k_bhsd = past_key

            v_bhsd = past_value if past_value.shape[1] == s else past_value.permute(0, 2, 1)
            past_key = k_bhsd.view(b, h, s, d)
            past_value = v_bhsd.view(b, h, s, d)

        b, h, s, d = past_key.shape
        past_k_new = past_key.permute(2, 0, 1, 3).reshape(s, b * h, d)
        past_v_new = past_value.permute(2, 0, 1, 3).reshape(s, b * h, d)
        return past_k_new, past_v_new

    def _to_torch_tensor(self, x):
        """Support compressed KV in FlexGen."""
        try:
            from bloombee.flexgen_utils.pytorch_backend import DeviceType
            if hasattr(x, 'device'):
                dev = getattr(x, 'device', None)
                if getattr(getattr(dev, 'device_type', None), '__eq__', lambda _: False)(DeviceType.COMPRESSED) \
                   or (hasattr(x, 'data') and isinstance(getattr(x, 'data'), tuple) and len(getattr(x, 'data')) == 3):
                    return x.device.decompress(x)
            return getattr(x, 'data', x)
        except Exception:
            return getattr(x, 'data', x)

    # ---- weight/cache helpers ----

    def load_weight(self, i, j, k, overlap=True):
        if overlap and self.load_weight_stream is not None:
            with torch.cuda.stream(self.load_weight_stream):
                self.layers[j].load_weight(self.weight_home[j], self.weight_read_buf[j], k)
        else:
            self.layers[j].load_weight(self.weight_home[j], self.weight_read_buf[j], k)

    def delete_weight(self, j, k):
        if k == 0:
            for x in self.weight_home[j].pop():
                if isinstance(x, ValueHolder):
                    for y in x.pop():
                        y.delete()
                else:
                    x.delete()

    def init_cache(self, j, k):
        pass

    def load_cache(self, i, j, k, overlap=True):
        if i == 0:
            return
        if overlap and self.load_cache_stream is not None:
            with torch.cuda.stream(self.load_cache_stream):
                self.layers[j].load_cache(self.cache_home[j][k], self.cache_read_buf[j][k], i)
        else:
            self.layers[j].load_cache(self.cache_home[j][k], self.cache_read_buf[j][k], i)

    def store_cache(self, i, j, k, overlap=True):
        if k == -1:
            k = self.num_gpu_batches - 1
            j -= 1
        if j == -1:
            j = self.num_layers - 1
            i -= 1
            if i == -1:
                return
        if overlap and self.store_cache_stream is not None:
            with torch.cuda.stream(self.store_cache_stream):
                self.layers[j].store_cache(self.cache_home[j][k], self.cache_write_buf[j][k], i)
        else:
            self.layers[j].store_cache(self.cache_home[j][k], self.cache_write_buf[j][k], i)

    def delete_cache(self, j, k):
        v = self.cache_home[j][k].pop()
        if v:
            for x in v:
                x.delete()

    def load_hidden(self, i, j, k):
        if k == self.num_gpu_batches:
            k = 0
            j += 1
        if j == self.num_layers:
            j = 0
            i += 1
            if i == self.execute_gen_len:
                return

        dst = self.layers[j].compute
        if j == 0:
            gpu_batch_size = self.policy.gpu_batch_size
            left, right = k * gpu_batch_size, (k + 1) * gpu_batch_size
            if i == 0:
                val = dst.allocate((gpu_batch_size, self.task.prompt_len), np.int32)
                val.load_from_np(self.output_ids[left:right, :self.task.prompt_len])
            else:
                pos = self.task.prompt_len + i
                val = dst.allocate((gpu_batch_size, 1), np.int32)
                val.load_from_np(self.output_ids[left:right, pos - 1:pos])
        else:
            val = self.hidden[0][j - 1][k].pop().move(dst)

        self.hidden[0][j][k].store(val)

    def load_hidden_mlp(self, i, j, k):
        self.hidden[0][j][k].store(self.temp_hidden.val)

    def store_hidden(self, i, j, k):
        if k == -1:
            k = self.num_gpu_batches - 1
            j -= 1
        if j == -1:
            j = self.num_layers - 1
            i -= 1
            if i == -1:
                return

        if j == self.num_layers - 1:
            gpu_batch_size = self.policy.gpu_batch_size
            left, right = k * gpu_batch_size, (k + 1) * gpu_batch_size
            ids = self.hidden[0][j][k].pop().data.detach().cpu().numpy()
            pos = self.task.prompt_len + i
            if self.task.stop:
                stopped = self.stopped[left:right]
                self.output_ids[left:right, pos:pos + 1] = np.where(
                    stopped, self.config.pad_token_id, ids)
                stopped[:] = np.logical_or(stopped, ids == self.task.stop)
            else:
                self.output_ids[left:right, pos:pos + 1] = ids
        else:
            x = self.hidden[0][j][k]
            if x.val:
                x.val = x.val.move(self.act_home)


class Wrapped{{ M }}Block(Optimized{{ M }}DecoderLayer):
    """
    Thin wrapper to:
      - translate external layer_past if needed
      - (optionally) build HF-style 4D causal masks
      - delegate to Optimized{{ M }}DecoderLayer.forward
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Pre-allocate attention_mask cache
        self._attention_mask_cache = {}

    def forward(
        self,
        hidden_states: torch.Tensor,
        *args,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        layer_past: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
        **kwargs,
    ):
        batch_size, seq_length, _ = hidden_states.shape
        seq_length_with_past = seq_length
        past_key_values_length = 0

        past_key_value = layer_past
        if past_key_value is not None:
            past_key_values_length = past_key_value[0].shape[2]
            seq_length_with_past += past_key_values_length
            {% if needs_bloom_to_llama_reorder %}
            past_key_value = self._reorder_cache_from_bloom_to_llama(past_key_value, batch_size, past_key_values_length)
            {% endif %}

        {% if prepare_mask_with_hf %}
        # Optimized: Reuse cached attention_mask
        if attention_mask is None:
            cache_key = (batch_size, seq_length, past_key_values_length, hidden_states.device, hidden_states.dtype)
            if cache_key not in self._attention_mask_cache:
                base_mask = torch.ones(
                    (batch_size, seq_length), dtype=torch.bool, device=hidden_states.device
                )
                self._attention_mask_cache[cache_key] = _prepare_4d_causal_attention_mask(
                    attention_mask=base_mask,
                    input_shape=(batch_size, seq_length),
                    inputs_embeds=hidden_states,
                    past_key_values_length=past_key_values_length,
                )
            attention_mask = self._attention_mask_cache[cache_key]
        else:
            attention_mask = _prepare_4d_causal_attention_mask(
                attention_mask=attention_mask,
                input_shape=(batch_size, seq_length),
                inputs_embeds=hidden_states,
                past_key_values_length=past_key_values_length,
            )
        {% endif %}

        return super().forward(
            hidden_states,
            *args,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            use_cache=use_cache,
            **kwargs,
        )

    {% if needs_bloom_to_llama_reorder %}
    def _reorder_cache_from_bloom_to_llama(
        self, key_value: Tuple[torch.Tensor], batch_size: int, seq_length: int
    ) -> Tuple[torch.Tensor]:
        """Reorder packed past KV into [B, H, S, D] for LLaMA."""
        key_states, value_states = key_value
        if key_states.dim() == 4 and value_states.dim() == 4:
            return key_states, value_states
        if key_states.dim() == 3:
            bh, d1, d2 = key_states.shape
            if d2 == self.self_attn.head_dim and d1 == seq_length:
                key_bhsd = key_states
            elif d1 == self.self_attn.head_dim and d2 == seq_length:
                key_bhsd = key_states.permute(0, 2, 1)
            else:
                key_bhsd = key_states.permute(0, 2, 1)

            if value_states.shape[1] == seq_length:
                val_bhsd = value_states
            else:
                val_bhsd = value_states.permute(0, 2, 1)

            h = self.self_attn.num_key_value_heads
            d = self.self_attn.head_dim
            key_out = key_bhsd.view(batch_size, h, seq_length, d)
            val_out = val_bhsd.view(batch_size, h, seq_length, d)
            return (key_out, val_out)
        return key_states, value_states

    def _reorder_cache_from_llama_to_bloom(
        self, key_value: Tuple[torch.Tensor], batch_size: int, seq_length: int
    ) -> Tuple[torch.Tensor]:
        key_states, value_states = key_value
        value_states = value_states.view(
            batch_size * self.self_attn.num_key_value_heads, seq_length, self.self_attn.head_dim
        )
        key_states = key_states.view(*value_states.shape)
        key_states = key_states.permute(0, 2, 1)
        return (key_states, value_states)
    {% endif %}
